# -*- coding: utf-8 -*-
"""Untitled379.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15IQZkJuOJz-6cKkdWFqSoYggH4uuv8F8
"""

import cv2
image_matrix = cv2.imread("/content/2.png")
image_vector = image_matrix.flatten()
print("Matrix:",image_matrix.shape)
print("Vector:",image_vector.shape)

sample_text = "من عاشق دانش آموز هام هستم."
print(sample_text)

# word2vec
# tokenaztion
sample_text.split(" ")

import nltk
nltk.download('punkt_tab')

import nltk
nltk.download('punkt')
from nltk.tokenize import word_tokenize
tokens = word_tokenize(sample_text)
print(tokens)

!pip install hazm

from hazm import word_tokenize
sample_text = "من عاشق محمد هستم"
tokens = word_tokenize(sample_text)
print(tokens)

vocabes = {}

# token to number
for index, token in enumerate(tokens):
  print(index,token)
  vocabes[token] = index

vocabes

from collections import Counter

# Sample list of tokenized sentences
tokenized_sentences = [
    ["hello", "how", "are", "you"],
    ["i", "am", "fine", "how", "about", "you","is"],
    ["hello", "i", "am", "glad","hello","ali"]
]

# Flatten all tokens into a single list
all_tokens = [token for sentence in tokenized_sentences for token in sentence]

# Count token frequencies
token_freq = Counter(all_tokens)

# Create vocabulary with indices (starting from 2 to reserve 0 and 1 for special tokens)
vocab = {token: idx+2 for idx, (token, _) in enumerate(token_freq.most_common())}

# Add special tokens
vocab["<PAD>"] = 0
vocab["<UNK>"] = 1

# Reverse vocabulary (optional, for decoding)
inv_vocab = {idx: token for token, idx in vocab.items()}

print("Vocabulary:", vocab)

text= "ali is good"
tokens = word_tokenize(text)
tokens

numbers = []
for token in tokens:
  if token not in vocab.keys():
    print(token , "-1")
    numbers.append(-1)
  else:
    print(token,vocab[token])
    numbers.append(vocab[token])

numbers





from gensim.models import Word2Vec
from gensim.utils import simple_preprocess

simple_preprocess("I love natural language processing")

# Sample corpus
sentences = [
    simple_preprocess("I love natural language processing"),
    simple_preprocess("Word2Vec my heart creates vector representations of words")
]

# Train Word2Vec model
model = Word2Vec(sentences=sentences, vector_size=128, window=5, min_count=1, sg=1)  # sg=1 for Skip-Gram

# Get word vector
vector = model.wv['love']

vector

similar = model.wv.most_similar('love')
similar



from gensim.models import FastText

# Same corpus
sentences = [
    simple_preprocess("I love natural language processing"),
    simple_preprocess("FastText works with subword information")
]

# Train FastText model
model = FastText(sentences=sentences, vector_size=100, window=5, min_count=1, sg=1)

# Get vector for known or OOV word
vector = model.wv['language']           # Known
vector_oov = model.wv['languages']      # May be OOV, still works

# Similar words
similar = model.wv.most_similar('language')

import matplotlib.pyplot as plt
from sklearn.manifold import TSNE
import numpy as np

def plot_tsne_embeddings(model, title="t-SNE Embedding Space", top_n=30):
    words = list(model.wv.key_to_index)[:top_n]
    word_vectors = np.array([model.wv[word] for word in words])

    # t-SNE for 2D projection
    tsne = TSNE(n_components=2, learning_rate='auto', init='pca', perplexity=5, random_state=42)
    reduced = tsne.fit_transform(word_vectors)

    # Plotting
    plt.figure(figsize=(10, 7))
    for i, word in enumerate(words):
        x, y = reduced[i]
        plt.scatter(x, y)
        plt.text(x + 0.01, y + 0.01, word, fontsize=9)
    plt.title(title)
    plt.grid(True)
    plt.show()

from gensim.models import Word2Vec, FastText
from gensim.utils import simple_preprocess

# Sample corpus
sentences = [
    simple_preprocess("Machine learning is fascinating"),
    simple_preprocess("Natural language processing uses machine learning"),
    simple_preprocess("Deep learning powers artificial intelligence"),
    simple_preprocess("FastText and Word2Vec are word embedding models")
]

# Word2Vec
w2v_model = Word2Vec(sentences=sentences, vector_size=100, window=5, min_count=1, sg=1)

# FastText
ft_model = FastText(sentences=sentences, vector_size=100, window=5, min_count=1, sg=1)

# Plot Word2Vec
plot_tsne_embeddings(w2v_model, title="Word2Vec Embeddings")

# Plot FastText
plot_tsne_embeddings(ft_model, title="FastText Embeddings")

from gensim.models import FastText, KeyedVectors

# Gensim provides some example models
model = KeyedVectors.load_word2vec_format('wiki.simple.vec')  # or use another .vec file

from transformers import AutoModelForCausalLM, AutoTokenizer

modelpath = "Chain-GPT/Solidity-LLM"

tokenizer = AutoTokenizer.from_pretrained(modelpath)
model = AutoModelForCausalLM.from_pretrained(modelpath).to("cuda")

prompt = "Write a Solidity function to transfer tokens."
inputs = tokenizer(prompt, return_tensors="pt").to("cuda")

outputs = model.generate(**inputs, max_new_tokens=1400, pad_token_id=tokenizer.eos_token_id)
generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

print(generated_text)

